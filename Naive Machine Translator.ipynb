{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import scipy\n",
    "import sklearn\n",
    "from utils import cosine_similarity, get_dict, process_tweet\n",
    "import pickle\n",
    "import string\n",
    "from nltk.corpus import stopwords, twitter_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data')\n",
    "data_path = '/data'\n",
    "nltk.data.path.append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shahbaz\n",
      "[nltk_data]     Akhtar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\Shahbaz\n",
      "[nltk_data]     Akhtar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open('en_embeddings.p', 'rb'))\n",
    "fr_embeddings_subset = pickle.load(open('fr_embeddings.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English embeddings size:  6370\n",
      "French embeddings size:   5766\n",
      "Embedding dimensions:     300\n"
     ]
    }
   ],
   "source": [
    "print('English embeddings size: ', len(en_embeddings_subset.keys()))\n",
    "print('French embeddings size:  ', len(fr_embeddings_subset.keys()))\n",
    "print('Embedding dimensions:    ',len(en_embeddings_subset[list(en_embeddings_subset.keys())[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dictionaries mapping english to french words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN - FR train size:  5000\n",
      "EN - FR test size:  1500\n"
     ]
    }
   ],
   "source": [
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('EN - FR train size: ', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('EN - FR test size: ', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embedding matrices E, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, en_vecs, fr_vecs):\n",
    "    e = [] #list containing english vectors\n",
    "    f = [] #list containing french vectors\n",
    "    f_idx_word_map = []\n",
    "    \n",
    "    en_set = en_vecs.keys()\n",
    "    fr_set = fr_vecs.keys()\n",
    "    \n",
    "    fr_words = en_fr.values()\n",
    "    \n",
    "    \n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        \n",
    "        if en_word in en_set and fr_word in fr_set:\n",
    "            en_vec = en_vecs[en_word]\n",
    "            fr_vec = fr_vecs[fr_word]\n",
    "            f_idx_word_map.append(fr_word)\n",
    "            \n",
    "            \n",
    "            e.append(en_vec)\n",
    "            f.append(fr_vec)\n",
    "    E = np.vstack(e)\n",
    "    F = np.vstack(f)\n",
    "    \n",
    "    return E, F, f_idx_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, f_idx_word_map = get_matrices(en_fr_train, en_embeddings_subset, fr_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function: Modified Forbenius norm of the matrices E, F:\n",
    "##### Forbinius norm: ||E*R - F ||\n",
    "##### loss function: (1/m)* || E*R - F ||^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    diff = np.dot(X,R) - Y\n",
    "    squared_diff_sum = np.sum(diff**2)\n",
    "    loss = squared_diff_sum / m\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient of the loss function: (E^T)*(ER - F) * (2 / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    gradient = np.dot(X.T, np.dot(X, R) - Y) * (2/m)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_R(X, Y, train_steps = 100, learning_rate = 0.001):\n",
    "    np.random.seed(1927)\n",
    "    \n",
    "    R = np.random.randn(X.shape[1], X.shape[1])\n",
    "    \n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        gradient = compute_gradient(X,Y,R)\n",
    "        \n",
    "        R -=  learning_rate * gradient\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 2547.8574\n",
      "loss at iteration 25 is: 303.0439\n",
      "loss at iteration 50 is: 84.1080\n",
      "loss at iteration 75 is: 29.6959\n",
      "loss at iteration 100 is: 12.2631\n",
      "loss at iteration 125 is: 5.7500\n",
      "loss at iteration 150 is: 3.0422\n",
      "loss at iteration 175 is: 1.8223\n",
      "loss at iteration 200 is: 1.2364\n",
      "loss at iteration 225 is: 0.9395\n",
      "loss at iteration 250 is: 0.7821\n",
      "loss at iteration 275 is: 0.6950\n",
      "loss at iteration 300 is: 0.6452\n",
      "loss at iteration 325 is: 0.6156\n",
      "loss at iteration 350 is: 0.5977\n",
      "loss at iteration 375 is: 0.5864\n"
     ]
    }
   ],
   "source": [
    "R_train = get_R(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for translation embedding using k-NN\n",
    "##### using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(v, candidates, k = 1):\n",
    "    similarity_list = []\n",
    "    for row in candidates:\n",
    "        cos = cosine_similarity(v, row)\n",
    "        similarity_list.append(cos)\n",
    "        \n",
    "    sorted_indices = np.argsort(similarity_list)\n",
    "    k_idx = sorted_indices[-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    pred = np.dot(X, R)\n",
    "    \n",
    "    num_correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = knn(pred[i], Y)\n",
    "        \n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / len(pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.560\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, en_embeddings_subset, fr_embeddings_subset)\n",
    "acc = test_vocabulary(X_val, Y_val, R_train)\n",
    "print(f'Accuracy is {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' même chien était noirs'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    sentence = sentence.split()\n",
    "    translation = ''\n",
    "    for word in sentence:\n",
    "        \n",
    "        embd = en_embeddings_subset[word]\n",
    "        pred_embd_vec = np.dot(embd, R_train)\n",
    "        pred_embd_idx = knn(pred_embd_vec, Y_train)\n",
    "        translation += ' ' + f_idx_word_map[int(pred_embd_idx)]\n",
    "    return translation\n",
    "\n",
    "en_sentence = 'the cat was black'\n",
    "predict(en_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
